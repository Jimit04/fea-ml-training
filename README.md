# FEA Reduced-Order Model (ROM)

A proof-of-concept that uses **Machine Learning to replace expensive FEA solvers**.
A neural network is trained on data generated from Euler-Bernoulli beam theory, then used to predict full displacement and stress fields in milliseconds.

> ğŸ“„ See [docs/Overview.md](docs/Overview.md) for a detailed explanation of the physics, architecture, and training strategy.
> ğŸ“ See [docs/GCNs.md](docs/GCNs.md) for an in-depth primer on Graph Convolutional Networks.

---

## Workflow

1. **Data Generation** â€” `MockFEASolver` analytically solves a cantilever beam using Euler-Bernoulli beam theory (Steel: E=210,000 MPa, Î½=0.29). Outputs displacement and stress fields on a 21Ã—6Ã—6 hex mesh.
2. **Training** â€” A TensorFlow/Keras neural network (MLP or GCN) learns the mapping from `[Length, Width, Depth, Load]` â†’ full field results.
3. **Visualisation** â€” PyVista renders GT vs Predicted side-by-side with max values and % errors.

---

## Setup

Project is managed with `uv`.

```bash
uv sync
```

---

## Usage

```bash
# Check for full help on usage
uv run .\main.py --help

```

## Models

### MLP (Multi-Layer Perceptron)

Deep Dense network: `4 â†’ 256 â†’ 512 â†’ 512 â†’ 256 â†’ output`

- Swish (SiLU) activations, BatchNorm, Dropout (15%)
- Compiled with Adam (lr=1e-3) and MSE loss

### GCN (Graph Convolutional Network)

Treats the beam mesh as a graph. Performs spectral convolution over the 21Ã—6Ã—6 node adjacency.

- Lifts global params â†’ per-node features via `RepeatVector(756)` + `Dense(32)`
- 6 GCN message-passing layers (`GCNLayer(128)`) with alternating ReLU / LeakyReLU
- `GlobalAveragePooling1D` â†’ Dense decoder head (256 â†’ 512 â†’ output)
- Compiled with Adam (lr=1e-3) and MSE loss

---

## Inputs & Physics

| Parameter | Symbol | Unit | Range       |
| --------- | ------ | ---- | ----------- |
| Length    | L      | mm   | 5 â€“ 20     |
| Width     | w      | mm   | 1 â€“ 3      |
| Depth     | d      | mm   | 1 â€“ 3      |
| Load      | P      | N    | -500 â€“ 500 |

**Material:** Steel â€” E = 210,000 MPa, Î½ = 0.29

**Mesh:** Structured hexahedral grid, 21 Ã— 6 Ã— 6 = 756 nodes

---

## Project Structure

```
fea-ml-training/
â”œâ”€â”€ main.py                    â† CLI entry point
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Overview.md            â† Detailed physics & architecture docs
â”‚   â””â”€â”€ GCNs.md                â† GCN deep-dive reference
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator.py      â† MockFEASolver (Euler-Bernoulli physics)
â”‚   â”œâ”€â”€ generate_dataset.py    â† Batch sample generation (multiple sampling strategies)
â”‚   â”œâ”€â”€ rom_model/             â† ROM model package
â”‚   â”‚   â”œâ”€â”€ __init__.py        â† Re-exports all public symbols
â”‚   â”‚   â”œâ”€â”€ layers.py          â† GCNLayer (custom Keras layer)
â”‚   â”‚   â”œâ”€â”€ adjacency.py       â† Normalised adjacency matrix builder
â”‚   â”‚   â”œâ”€â”€ architectures.py   â† MLP & GCN model factories
â”‚   â”‚   â””â”€â”€ trainer.py         â† ROMTrainer (training pipeline)
â”‚   â””â”€â”€ visualizer.py          â† PyVista 3D visualizer (predicted vs ground truth)
â”œâ”€â”€ tests/                     â† Batch scripts for end-to-end testing
â”œâ”€â”€ mock_data/<sampling>/      â† Generated .npy and .vtk samples
â””â”€â”€ models/<sampling>/         â† Saved .keras models and .npy scalers
```
